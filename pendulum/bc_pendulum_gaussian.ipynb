{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import time \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import gym \n",
    "import pickle  \n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions.transformed_distribution import TransformedDistribution\n",
    "from torch.distributions.transforms import TanhTransform\n",
    "import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Fix training issue, Doesn't work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, maxlen=1_000_000):  \n",
    "        self.states=None\n",
    "        self.actions=None\n",
    "        self.ids=None\n",
    "        self.maxlen=maxlen\n",
    "        \n",
    "    def add_data(self, states, actions):\n",
    "        \"\"\"\n",
    "        add N s,a,r\n",
    "        \"\"\"\n",
    "        if self.states is None: #first time\n",
    "            self.states =states[-self.maxlen:] \n",
    "            self.actions=actions[-self.maxlen:]\n",
    "        else:\n",
    "            self.states = np.concatenate([self.states, states])[-self.maxlen:] \n",
    "            self.actions = np.concatenate([self.actions, actions])[-self.maxlen:] \n",
    "        \n",
    "    def sample_batch(self, batch_size):\n",
    "        indices = np.random.permutation(len(self))[:batch_size]\n",
    "        batch_states=self.states[indices] \n",
    "        batch_actions=self.actions[indices]\n",
    "   \n",
    "        return batch_states, batch_actions \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, action_dim, state_dim, size=32):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.mlp = nn.ModuleList()\n",
    "        self.mlp.append(nn.Linear(state_dim, size))\n",
    "        self.mlp.append(nn.Tanh())\n",
    "\n",
    "        # self.mlp.append(nn.Linear(size, size))\n",
    "        # self.mlp.append(nn.Tanh()) \n",
    "\n",
    "        self.mu_head = nn.Linear(size, action_dim)\n",
    "        self.sigma_head = nn.Linear(size, action_dim)\n",
    "\n",
    "    def _get_outputs(self, x):\n",
    "        for layer in self.mlp:\n",
    "            x = layer(x)\n",
    "        \n",
    "        mu = self.mu_head(x)\n",
    "        # mu = torch.clip(mu, MEAN_MIN, MEAN_MAX)\n",
    "        log_sigma = self.sigma_head(x)\n",
    "        # log_sigma = torch.clip(log_sigma, LOG_STD_MIN, LOG_STD_MAX)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "\n",
    "        a_distribution = TransformedDistribution(\n",
    "            Normal(mu, sigma), TanhTransform(cache_size=1)\n",
    "        )\n",
    "        a_tanh_mode = torch.tanh(mu)\n",
    "        return a_distribution, a_tanh_mode\n",
    "    \n",
    "    def get_mean(self, x):\n",
    "        for layer in self.mlp:\n",
    "            x = layer(x)\n",
    "        \n",
    "        mu = self.mu_head(x)\n",
    "        return mu\n",
    "    \n",
    "    def get_action(self, x, deterministic=False):\n",
    "        a_dist, a_tanh_mode = self._get_outputs(x)\n",
    "        if deterministic:\n",
    "            action = a_tanh_mode\n",
    "        else:\n",
    "            action = a_dist.rsample()\n",
    "        return action\n",
    "    \n",
    "    def forward(self, state):\n",
    "        a_dist, a_tanh_mode = self._get_outputs(state)\n",
    "        action = a_dist.rsample()\n",
    "        logp_pi = a_dist.log_prob(action).sum(axis=-1)\n",
    "        return action, logp_pi, a_tanh_mode\n",
    "\n",
    "    def get_log_density(self, state, action):\n",
    "        a_dist, _ = self._get_outputs(state)\n",
    "        action_clip = torch.clip(action, -1. + EPS, 1. - EPS)\n",
    "        logp_action = a_dist.log_prob(action_clip)\n",
    "        return logp_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, actor, is_close=True, is_render=True, max_step=1000): \n",
    "    obs,info = env.reset()\n",
    "    dones=False\n",
    "    total_r=0\n",
    "    step=0\n",
    "    while not dones: \n",
    "        step+=1\n",
    "        obs=torch.Tensor(obs[None]).to(device)\n",
    "        action, logp_pi, a_tanh_mode = actor(obs)\n",
    "        # action=actor.get_mean(obs)\n",
    "        action=action.cpu().detach().numpy() \n",
    "\n",
    "        obs, rewards, done, s, info = env.step(action.ravel())\n",
    "        total_r +=rewards  \n",
    "        if done:\n",
    "            break\n",
    "        if step>max_step:\n",
    "            # print('max step reached')\n",
    "            break\n",
    "        # elif s:\n",
    "        #     print('solved!')\n",
    "        #     break\n",
    "    if is_close:\n",
    "        env.close()\n",
    "    return {'reward':total_r, 'step':step-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name='Pendulum-v1'\n",
    "data_path=\"/home/ns/bc_tutorial/pendulum/expert_data/Pendulum-v1_10_-130.pkl\" \n",
    "data_path = \"expert_data/Pendulum-v1_50_-149.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expert data loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4000, 3), (4000, 1), (4000, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(data_path, \"rb\") as f:\n",
    "    data_good = pickle.load(f)\n",
    "print('expert data loaded')\n",
    "\n",
    "data_good=data_good[:20]\n",
    "\n",
    "good_obs=[]\n",
    "good_acts=[]\n",
    "good_rews=[]\n",
    "\n",
    "for traj in data_good: \n",
    "    s,a,r=traj  \n",
    "\n",
    "    good_obs.append(s)\n",
    "    good_acts.append(a)\n",
    "    good_rews.append(r)\n",
    "\n",
    "good_obs=np.vstack(good_obs)\n",
    "good_acts=np.vstack(good_acts)\n",
    "good_rews=np.vstack(good_rews) \n",
    "\n",
    "good_obs.shape, good_acts.shape, good_rews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.9999993, 1.9999466, -0.29554793, 0.7047518)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(good_acts), np.max(good_acts), np.mean(good_acts), np.std(good_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.0, -0.1477628, 0.35238066)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_min=np.min(good_acts)\n",
    "action_max=np.max(good_acts)\n",
    "\n",
    "#convert to -1,1\n",
    "actions= -1 + (good_acts-action_min)/(action_max-action_min)*2\n",
    "\n",
    "np.min(actions), np.max(actions), np.mean(actions), np.std(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.9999993, 1.9999466)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_min, action_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "states=good_obs\n",
    "actions=actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=Memory(maxlen=1000000)\n",
    "memory.add_data(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed) \n",
    "env = gym.make(env_name) \n",
    "\n",
    "ob_dim = env.observation_space.shape[0]\n",
    "ac_dim = env.action_space.shape[0]\n",
    "\n",
    "actor = Actor(ac_dim, ob_dim).to(device)\n",
    " \n",
    "optimizer = torch.optim.Adam(actor.parameters(), 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1000  loss 0.26415225863456726\n",
      "step: 2000  loss 0.3189757168292999\n",
      "step: 3000  loss -0.06276030838489532\n",
      "step: 4000  loss -0.3269127607345581\n",
      "step: 5000  loss -0.22007445991039276\n",
      "step: 6000  loss -0.0013023471692577004\n",
      "step: 7000  loss -0.36797699332237244\n",
      "step: 8000  loss -0.26837703585624695\n",
      "step: 9000  loss -0.2672169506549835\n",
      "step: 10000  loss -0.2926677167415619\n",
      "bc training completed\n"
     ]
    }
   ],
   "source": [
    "verbose=True\n",
    "batch_size=100\n",
    "training_timestep=10000\n",
    "actor.train()\n",
    "for train_step in range(1, training_timestep+1):  \n",
    "    batch_states, batch_actions =memory.sample_batch(batch_size)  \n",
    "    batch_states = torch.Tensor(batch_states).to(device)\n",
    "    batch_actions=torch.Tensor(batch_actions).to(device)\n",
    "     \n",
    "    log_pi_e = actor.get_log_density(batch_states, batch_actions) \n",
    "    bc_loss = -torch.sum(log_pi_e, 1)\n",
    "    loss = torch.mean(bc_loss)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if verbose and (train_step % (training_timestep/10)==0):\n",
    "        print('step:', train_step, ' loss', loss.item()) \n",
    "\n",
    "print('bc training completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=torch.tensor(states, dtype=torch.float)\n",
    "train_y=torch.tensor(actions, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression score:  -4.046102046966553\n"
     ]
    }
   ],
   "source": [
    "yhat=actor.get_mean(train_x.to(device)).cpu().detach().numpy()\n",
    "\n",
    "action_unnorm = action_min + (yhat + 1.) *  (action_max - action_min) / (1+1)\n",
    "\n",
    "yhat=action_unnorm\n",
    "score = 1 - ( (actions - yhat )**2 ).sum() /( (actions - actions.mean() )**2 ).sum()\n",
    "print('regression score: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "obs,info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs=torch.Tensor(obs[None]).to(device)\n",
    "action, logp_pi, a_tanh_mode = actor(obs)\n",
    "# action=actor.get_mean(obs)\n",
    "action=action.cpu().detach().numpy() \n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8385]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.get_mean(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.901257300542163"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action=actor.get_action(obs, deterministic=True)[0][0].cpu().detach().numpy()\n",
    "action_unnorm = action_min + (action + 1.) *  (action_max - action_min) / (1+1)\n",
    "action_unnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, actor, is_close=True, is_render=True, max_step=1000): \n",
    "    obs,info = env.reset()\n",
    "    dones=False\n",
    "    total_r=0\n",
    "    step=0\n",
    "    while not dones: \n",
    "        step+=1\n",
    "        obs=torch.Tensor(obs[None]).to(device)\n",
    "        action=actor.get_action(obs, deterministic=True)[0][0].cpu().detach().numpy()\n",
    "        action_unnorm = action_min + (action + 1.) *  (action_max - action_min) / (1+1)\n",
    "        action=action_unnorm\n",
    "\n",
    "        obs, rewards, done, trunc, info = env.step([action] )\n",
    "        total_r +=rewards  \n",
    "        if done or trunc:\n",
    "            break\n",
    "    return {'reward':total_r, 'step':step-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reward': -1176.6829153347444, 'step': 199}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play(env, actor, is_close=True, is_render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:06<00:00,  7.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward=-1332.46 std=184.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rewards=[]\n",
    "for i in tqdm.tqdm( range(50) ):\n",
    "    stats=play(env, actor, is_close=True, is_render=False)\n",
    "    rewards.append(stats['reward'])\n",
    "    # print(f'i={i} reward={stats[\"reward\"]} step={stats[\"step\"]}')\n",
    "print(f'mean reward={np.mean(rewards) :.2f} std={np.std(rewards):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imitation-dice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
