{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pickle\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_size=64):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.mean = nn.Linear(hidden_size, output_dim)\n",
    "        self.log_std = nn.Parameter(-0.5 * torch.ones(output_dim))\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "output_dim = 1  \n",
    "hidden_size = 64\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize policy and optimizer\n",
    "policy = GaussianPolicy(input_dim, output_dim, hidden_size)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name='Pendulum-v1'\n",
    "data_path=\"/home/ns/bc_tutorial/pendulum/expert_data/Pendulum-v1_10_-130.pkl\" \n",
    "data_path = \"expert_data/Pendulum-v1_50_-149.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expert data loaded\n",
      "X: (4000, 3)  y: (4000, 1)\n"
     ]
    }
   ],
   "source": [
    "with open(data_path, \"rb\") as f:\n",
    "    data_good = pickle.load(f)\n",
    "print('expert data loaded')\n",
    "\n",
    "data_good=data_good[:20]\n",
    "\n",
    "good_obs=[]\n",
    "good_acts=[] \n",
    "for traj in data_good: \n",
    "    s,a,r=traj   \n",
    "    good_obs.append(s)\n",
    "    good_acts.append(a) \n",
    "\n",
    "states=np.vstack(good_obs)\n",
    "actions=np.vstack(good_acts)\n",
    "print('X:',states.shape,' y:', actions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 3]), torch.Size([64, 1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = torch.utils.data.DataLoader( list(zip(states, actions)), batch_size=64, shuffle=True)\n",
    "\n",
    "batch=next(iter(data_loader))\n",
    "states,actions = batch\n",
    "states.shape,actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.9916788339614868\n",
      "Epoch [2/100], Loss: 0.8724008798599243\n",
      "Epoch [3/100], Loss: 0.8098883032798767\n",
      "Epoch [4/100], Loss: 0.9554415941238403\n",
      "Epoch [5/100], Loss: 0.5279438495635986\n",
      "Epoch [6/100], Loss: 0.5528560280799866\n",
      "Epoch [7/100], Loss: 0.5995151400566101\n",
      "Epoch [8/100], Loss: 0.8981122970581055\n",
      "Epoch [9/100], Loss: 0.44481563568115234\n",
      "Epoch [10/100], Loss: 0.41651296615600586\n",
      "Epoch [11/100], Loss: 0.42773428559303284\n",
      "Epoch [12/100], Loss: 0.35740163922309875\n",
      "Epoch [13/100], Loss: 0.4800478219985962\n",
      "Epoch [14/100], Loss: 0.30677199363708496\n",
      "Epoch [15/100], Loss: 0.3473084270954132\n",
      "Epoch [16/100], Loss: 1.3640999794006348\n",
      "Epoch [17/100], Loss: 0.37510472536087036\n",
      "Epoch [18/100], Loss: 0.31631189584732056\n",
      "Epoch [19/100], Loss: 0.3687788248062134\n",
      "Epoch [20/100], Loss: 0.24650098383426666\n",
      "Epoch [21/100], Loss: 0.1555606722831726\n",
      "Epoch [22/100], Loss: 0.5347149968147278\n",
      "Epoch [23/100], Loss: 0.25805050134658813\n",
      "Epoch [24/100], Loss: 0.32518959045410156\n",
      "Epoch [25/100], Loss: 0.5708033442497253\n",
      "Epoch [26/100], Loss: 0.25655683875083923\n",
      "Epoch [27/100], Loss: 0.4985308349132538\n",
      "Epoch [28/100], Loss: 0.3708166182041168\n",
      "Epoch [29/100], Loss: 0.31037870049476624\n",
      "Epoch [30/100], Loss: 0.6458168029785156\n",
      "Epoch [31/100], Loss: 0.055566880851984024\n",
      "Epoch [32/100], Loss: 1.0829013586044312\n",
      "Epoch [33/100], Loss: 0.11949832737445831\n",
      "Epoch [34/100], Loss: 0.24115553498268127\n",
      "Epoch [35/100], Loss: 0.20241470634937286\n",
      "Epoch [36/100], Loss: 0.09132176637649536\n",
      "Epoch [37/100], Loss: -0.17125651240348816\n",
      "Epoch [38/100], Loss: 1.303635597229004\n",
      "Epoch [39/100], Loss: -0.0972246304154396\n",
      "Epoch [40/100], Loss: -0.08281144499778748\n",
      "Epoch [41/100], Loss: -0.22373536229133606\n",
      "Epoch [42/100], Loss: 1.2007620334625244\n",
      "Epoch [43/100], Loss: 0.48806673288345337\n",
      "Epoch [44/100], Loss: -0.19222769141197205\n",
      "Epoch [45/100], Loss: -0.2936457395553589\n",
      "Epoch [46/100], Loss: -0.07210791110992432\n",
      "Epoch [47/100], Loss: 0.0646466612815857\n",
      "Epoch [48/100], Loss: 0.2444969117641449\n",
      "Epoch [49/100], Loss: -0.08131363987922668\n",
      "Epoch [50/100], Loss: -0.1609734743833542\n",
      "Epoch [51/100], Loss: -0.30139583349227905\n",
      "Epoch [52/100], Loss: 0.25875934958457947\n",
      "Epoch [53/100], Loss: -0.2974068820476532\n",
      "Epoch [54/100], Loss: -0.4124439060688019\n",
      "Epoch [55/100], Loss: -0.12934227287769318\n",
      "Epoch [56/100], Loss: -0.20819908380508423\n",
      "Epoch [57/100], Loss: -0.24879828095436096\n",
      "Epoch [58/100], Loss: -0.1402444839477539\n",
      "Epoch [59/100], Loss: -0.15129798650741577\n",
      "Epoch [60/100], Loss: -0.3388148546218872\n",
      "Epoch [61/100], Loss: 1.0981979370117188\n",
      "Epoch [62/100], Loss: -0.3768693208694458\n",
      "Epoch [63/100], Loss: -0.42591530084609985\n",
      "Epoch [64/100], Loss: -0.23681537806987762\n",
      "Epoch [65/100], Loss: -0.45265406370162964\n",
      "Epoch [66/100], Loss: -0.33506226539611816\n",
      "Epoch [67/100], Loss: 1.9398020505905151\n",
      "Epoch [68/100], Loss: -0.18514303863048553\n",
      "Epoch [69/100], Loss: -0.3675701916217804\n",
      "Epoch [70/100], Loss: -0.0967300534248352\n",
      "Epoch [71/100], Loss: -0.5349797606468201\n",
      "Epoch [72/100], Loss: -0.1708318293094635\n",
      "Epoch [73/100], Loss: 0.9813032150268555\n",
      "Epoch [74/100], Loss: -0.4522019028663635\n",
      "Epoch [75/100], Loss: -0.43438899517059326\n",
      "Epoch [76/100], Loss: -0.5212897062301636\n",
      "Epoch [77/100], Loss: -0.3890233635902405\n",
      "Epoch [78/100], Loss: -0.33371126651763916\n",
      "Epoch [79/100], Loss: -0.402675062417984\n",
      "Epoch [80/100], Loss: 0.5079218149185181\n",
      "Epoch [81/100], Loss: -0.41976234316825867\n",
      "Epoch [82/100], Loss: -0.3743937611579895\n",
      "Epoch [83/100], Loss: 0.19812363386154175\n",
      "Epoch [84/100], Loss: -0.0405544638633728\n",
      "Epoch [85/100], Loss: -0.521569013595581\n",
      "Epoch [86/100], Loss: -0.4498569071292877\n",
      "Epoch [87/100], Loss: -0.35930922627449036\n",
      "Epoch [88/100], Loss: -0.2026226818561554\n",
      "Epoch [89/100], Loss: -0.14262589812278748\n",
      "Epoch [90/100], Loss: -0.3534166216850281\n",
      "Epoch [91/100], Loss: -0.17260432243347168\n",
      "Epoch [92/100], Loss: -0.6192756295204163\n",
      "Epoch [93/100], Loss: -0.38706499338150024\n",
      "Epoch [94/100], Loss: 2.165964365005493\n",
      "Epoch [95/100], Loss: -0.02290293574333191\n",
      "Epoch [96/100], Loss: -0.6984370350837708\n",
      "Epoch [97/100], Loss: -0.4964989423751831\n",
      "Epoch [98/100], Loss: -0.5815406441688538\n",
      "Epoch [99/100], Loss: -0.49009501934051514\n",
      "Epoch [100/100], Loss: -0.32190215587615967\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs): \n",
    "    for states, actions in data_loader: \n",
    "        \n",
    "        # Forward pass\n",
    "        means, stds = policy(states)\n",
    "        dist = Normal(means, stds)\n",
    "        log_probs = dist.log_prob(actions).sum(dim=-1)\n",
    "        loss = -log_probs.mean()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Save trained policy\n",
    "torch.save(policy.state_dict(), 'pendulum_policy.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.99960434,  0.02812642, -0.7410221 ], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "obs,info = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states=torch.tensor(obs[None], dtype=torch.float)\n",
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs,info = env.reset()\n",
    "dones=False\n",
    "total_r=0\n",
    "step=0\n",
    "while not dones: \n",
    "    step+=1\n",
    "    states=torch.tensor(obs[None], dtype=torch.float)\n",
    "    means, stds = policy(states) \n",
    "    action=means[0][0].detach().numpy()\n",
    "\n",
    "    obs, rewards, done, trunc, info = env.step([action] )\n",
    "    total_r +=rewards  \n",
    "    if done or trunc:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.1492611795627912"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, policy, is_close=True, is_render=True, max_step=1000): \n",
    "    obs,info = env.reset()\n",
    "    dones=False\n",
    "    total_r=0\n",
    "    step=0\n",
    "    while not dones: \n",
    "        step+=1\n",
    "        states=torch.tensor(obs[None], dtype=torch.float)\n",
    "        means, stds = policy(states) \n",
    "        action=means[0][0].detach().numpy() \n",
    "\n",
    "        obs, rewards, done, trunc, info = env.step([action] )\n",
    "        total_r +=rewards  \n",
    "        if done or trunc:\n",
    "            break\n",
    "    if is_close or is_render:\n",
    "        env.close()\n",
    "    return {'reward':total_r, 'step':step-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reward': -232.26774121543895, 'step': 199}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env=gym.make(env_name)\n",
    "stats=play(env, policy, is_close=True, is_render=False)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode #0 reward: -1.47\n",
      "episode #1 reward: -126.49\n",
      "episode #2 reward: -1.50\n",
      "episode #3 reward: -1.30\n",
      "episode #4 reward: -119.41\n",
      "episode #5 reward: -250.78\n",
      "episode #6 reward: -224.71\n",
      "episode #7 reward: -125.62\n",
      "episode #8 reward: -236.14\n",
      "episode #9 reward: -124.98\n",
      "episode #10 reward: -1.68\n",
      "episode #11 reward: -376.89\n",
      "episode #12 reward: -1.06\n",
      "episode #13 reward: -248.21\n",
      "episode #14 reward: -258.34\n",
      "episode #15 reward: -234.08\n",
      "episode #16 reward: -333.04\n",
      "episode #17 reward: -116.63\n",
      "episode #18 reward: -119.59\n",
      "episode #19 reward: -119.33\n",
      "\n",
      " score: -151.06 +- 112.40\n"
     ]
    }
   ],
   "source": [
    "scores=[]\n",
    "n_trajectory=20\n",
    "for i in range(n_trajectory):\n",
    "    stats=play(env, policy, is_close=True, is_render=False)\n",
    "    rewards=stats['reward']\n",
    "    print(f'episode #{i} reward: {rewards:0.2f}')\n",
    "    scores.append(rewards)\n",
    "\n",
    "print(f'\\n score: {np.mean(scores):0.2f} +- {np.std(scores):0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imitation-dice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
