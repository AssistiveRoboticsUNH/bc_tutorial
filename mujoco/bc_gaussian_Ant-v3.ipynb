{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import time \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import gym \n",
    "import pickle \n",
    "from replay_memory import Memory\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions.transformed_distribution import TransformedDistribution\n",
    "from torch.distributions.transforms import TanhTransform\n",
    "import tqdm\n",
    "\n",
    "from nets import Actor_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name='Ant-v3'\n",
    "data_path = \"/home/ns/bc_tutorial/mujoco/expert_data/Ant-v3_10_3765.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expert data loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000, 111), (10000, 8), (10000, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(data_path, \"rb\") as f:\n",
    "    data_good = pickle.load(f)\n",
    "print('expert data loaded')\n",
    "\n",
    "data_good=data_good[:20]\n",
    "\n",
    "good_obs=[]\n",
    "good_acts=[]\n",
    "good_rews=[]\n",
    "\n",
    "for traj in data_good: \n",
    "    s,a,r=traj  \n",
    "\n",
    "    good_obs.append(s)\n",
    "    good_acts.append(a)\n",
    "    good_rews.append(r)\n",
    "\n",
    "good_obs=np.vstack(good_obs)\n",
    "good_acts=np.vstack(good_acts)\n",
    "good_rews=np.vstack(good_rews) \n",
    "\n",
    "good_obs.shape, good_acts.shape, good_rews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=Memory(maxlen=1000000)\n",
    "memory.add_data(good_obs, good_acts, good_rews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ns/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment Ant-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/ns/anaconda3/lib/python3.9/site-packages/gym/envs/mujoco/mujoco_env.py:190: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "seed=0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed) \n",
    "env = gym.make(env_name) \n",
    "\n",
    "ob_dim = env.observation_space.shape[0]\n",
    "ac_dim = env.action_space.shape[0]\n",
    "\n",
    "actor = Actor_D(ac_dim, ob_dim).to(device)\n",
    " \n",
    "optimizer = torch.optim.Adam(actor.parameters(), 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1000  loss -4.902913570404053\n",
      "step: 2000  loss -6.244664669036865\n",
      "step: 3000  loss -7.190088748931885\n",
      "step: 4000  loss -7.46046257019043\n",
      "step: 5000  loss -7.884944915771484\n",
      "step: 6000  loss -7.924429893493652\n",
      "step: 7000  loss -8.098227500915527\n",
      "step: 8000  loss -8.21967887878418\n",
      "step: 9000  loss -8.855413436889648\n",
      "step: 10000  loss -8.62328815460205\n",
      "step: 11000  loss -9.741211891174316\n",
      "step: 12000  loss -8.712021827697754\n",
      "step: 13000  loss -8.523443222045898\n",
      "step: 14000  loss -9.51380443572998\n",
      "step: 15000  loss -9.348519325256348\n",
      "step: 16000  loss -9.608366012573242\n",
      "step: 17000  loss -9.730209350585938\n",
      "step: 18000  loss -9.397101402282715\n",
      "step: 19000  loss -10.095282554626465\n",
      "step: 20000  loss -9.801156044006348\n",
      "step: 21000  loss -10.115110397338867\n",
      "step: 22000  loss -10.215883255004883\n",
      "step: 23000  loss -10.161785125732422\n",
      "step: 24000  loss -9.732927322387695\n",
      "step: 25000  loss -9.744319915771484\n",
      "step: 26000  loss -10.043241500854492\n",
      "step: 27000  loss -10.586008071899414\n",
      "step: 28000  loss -9.79404354095459\n",
      "step: 29000  loss -9.98785400390625\n",
      "step: 30000  loss -10.482471466064453\n",
      "step: 31000  loss -11.100668907165527\n",
      "step: 32000  loss -10.68882942199707\n",
      "step: 33000  loss -10.993979454040527\n",
      "step: 34000  loss -9.931315422058105\n",
      "step: 35000  loss -10.557543754577637\n",
      "step: 36000  loss -10.639707565307617\n",
      "step: 37000  loss -10.468631744384766\n",
      "step: 38000  loss -10.903661727905273\n",
      "step: 39000  loss -10.706430435180664\n",
      "step: 40000  loss -10.56589126586914\n",
      "step: 41000  loss -10.899271965026855\n",
      "step: 42000  loss -11.416033744812012\n",
      "step: 43000  loss -10.726947784423828\n",
      "step: 44000  loss -11.060461044311523\n",
      "step: 45000  loss -10.812146186828613\n",
      "step: 46000  loss -10.61141586303711\n",
      "step: 47000  loss -11.287467956542969\n",
      "step: 48000  loss -11.335822105407715\n",
      "step: 49000  loss -11.07433795928955\n",
      "step: 50000  loss -11.632172584533691\n",
      "bc training completed\n"
     ]
    }
   ],
   "source": [
    "verbose=True\n",
    "batch_size=100\n",
    "training_timestep=10000\n",
    "actor.train()\n",
    "for train_step in range(1, 50000+1):  \n",
    "    batch_states, batch_actions, batch_rewards=memory.sample_batch(batch_size)  \n",
    "    batch_states = torch.Tensor(batch_states).to(device)\n",
    "    batch_actions=torch.Tensor(batch_actions).to(device)\n",
    "     \n",
    "    log_pi_e = actor.get_log_density(batch_states, batch_actions) \n",
    "    bc_loss = -torch.sum(log_pi_e, 1)\n",
    "    loss = torch.mean(bc_loss)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if verbose and (train_step % (training_timestep/10)==0):\n",
    "        print('step:', train_step, ' loss', loss.item()) \n",
    "\n",
    "print('bc training completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, policy, is_close=True, is_render=True, max_step=1000): \n",
    "    obs,info = env.reset()\n",
    "    dones=False\n",
    "    total_r=0\n",
    "    step=0\n",
    "    while not dones: \n",
    "        step+=1\n",
    "        obs=torch.Tensor(obs[None]).to(device)\n",
    "        # action, logp_pi, a_tanh_mode = actor(obs)\n",
    "        action=actor.get_mean(obs)\n",
    "        action=action.cpu().detach().numpy() \n",
    "\n",
    "        obs, rewards, done, s, info = env.step(action.ravel())\n",
    "        total_r +=rewards  \n",
    "        if done:\n",
    "            break\n",
    "        if step>max_step:\n",
    "            # print('max step reached')\n",
    "            break\n",
    "        # elif s:\n",
    "        #     print('solved!')\n",
    "        #     break\n",
    "    if is_close:\n",
    "        env.close()\n",
    "    return {'reward':total_r, 'step':step-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ns/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment Ant-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/ns/anaconda3/lib/python3.9/site-packages/gym/envs/mujoco/mujoco_env.py:190: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env_name='Ant-v3' \n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reward': 4121.5574559161805, 'step': 1000}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "play(env, actor, is_close=True, is_render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:38<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward=3967.68 std=418.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rewards=[]\n",
    "for i in tqdm.tqdm( range(50) ):\n",
    "    stats=play(env, actor, is_close=True, is_render=False)\n",
    "    rewards.append(stats['reward'])\n",
    "    # print(f'i={i} reward={stats[\"reward\"]} step={stats[\"step\"]}')\n",
    "print(f'mean reward={np.mean(rewards) :.2f} std={np.std(rewards):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward=3967.68 std=418.52\n"
     ]
    }
   ],
   "source": [
    "print(f'mean reward={np.mean(rewards) :.2f} std={np.std(rewards):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imitation-dice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
