{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### installation for colab\n",
    "To Run in Colab: uncomment and run the following\n",
    "\n",
    "[Open In Colab](https://colab.research.google.com/github/AssistiveRoboticsUNH/bc_tutorial/blob/main/robomimic_tasks/train_lift_dag.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/ARISE-Initiative/robomimic\n",
    "# !pip install -e robomimic/\n",
    "\n",
    "# import sys\n",
    "# import os\n",
    "# sys.path.append('./robomimic/')\n",
    "\n",
    "\n",
    "\n",
    "# # install all system dependencies for mujoco-py\n",
    "# !sudo apt install curl git libgl1-mesa-dev libgl1-mesa-glx libglew-dev \\\n",
    "#          libosmesa6-dev software-properties-common net-tools unzip vim \\\n",
    "#          virtualenv wget xserver-xorg-dev libglfw3-dev patchelf\n",
    "\n",
    "# #install mujoco-py\n",
    "# !pip install mujoco\n",
    "\n",
    "# #install robosuite\n",
    "# !pip install robosuite==1.4.1\n",
    "\n",
    "\n",
    "# # download lift ph dataset.\n",
    "# !wget http://downloads.cs.stanford.edu/downloads/rt_benchmark/lift/ph/low_dim_v141.hdf5 -O lift_ph_low_dim_v141.hdf5\n",
    "\n",
    "# # download lift mh dataset\n",
    "# !wget http://downloads.cs.stanford.edu/downloads/rt_benchmark/lift/mh/low_dim_v141.hdf5 -O lift_mh_low_dim_v141.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No private macro file found!\n",
      "    It is recommended to use a private macro file\n",
      "    To setup, run: python /home/ns/mimicgen/mimicgen_envs/robomimic/robomimic/scripts/setup_macros.py\n",
      ")\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import robomimic\n",
    "import robomimic.utils.file_utils as FileUtils\n",
    "import robomimic.utils.env_utils as EnvUtils\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "import imageio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.optim import Adam \n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device= torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Robomimic Lift Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of demos: 300\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"lift_mh_low_dim_v141.hdf5\"\n",
    "\n",
    "f = h5py.File(dataset_path, \"r\")\n",
    "demos = list(f[\"data\"].keys())\n",
    "num_demos = len(demos)\n",
    "print(f'Number of demos: {num_demos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['demo_50',\n",
       " 'demo_51',\n",
       " 'demo_52',\n",
       " 'demo_53',\n",
       " 'demo_54',\n",
       " 'demo_55',\n",
       " 'demo_56',\n",
       " 'demo_57',\n",
       " 'demo_58',\n",
       " 'demo_59']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worse_demo_names = [ b.decode('utf-8') for b in f['mask']['worse_operator_2'] ] \n",
    "demos = worse_demo_names[:10]\n",
    "demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_keys=['object', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1688, 19), (1688, 7))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_org_states=[]\n",
    "D_org_actions=[]\n",
    "\n",
    "for demo_name in demos:  #  demo names\n",
    "    traj=f['data'][demo_name]\n",
    " \n",
    "    actions=traj['actions']\n",
    "    select_obs=np.hstack( [traj['obs'][key] for key in select_keys] ) \n",
    "    D_org_actions.append(actions)\n",
    "    D_org_states.append(select_obs)\n",
    "\n",
    "D_org_actions=np.concatenate(D_org_actions)\n",
    "D_org_states=np.concatenate(D_org_states)\n",
    "D_org_states.shape, D_org_actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, size=32):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim,size),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(size,size),\n",
    "            nn.ReLU() \n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "    \n",
    "class RegNet(MLP):\n",
    "    def __init__(self, input_dim , size, action_dim):\n",
    "        super(RegNet, self).__init__(input_dim, size)\n",
    "        self.decoder = nn.Linear(size, action_dim)\n",
    "    def forward(self,x):\n",
    "        x = self.net(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "\n",
    "state_dim = 19 \n",
    "action_dim=7\n",
    "\n",
    "bc = RegNet(state_dim, 64, action_dim)\n",
    "criterion = nn.MSELoss() \n",
    "optimizer = Adam(bc.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, states, actions, batch_size=64, n_epoch = 3000, print_every=100):\n",
    "\n",
    "    data_loader = DataLoader( list(zip(states, actions)), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    loss_list = []  \n",
    "    \n",
    "    for itr in range(0, n_epoch+1):\n",
    "        total_loss = 0\n",
    "        b=0\n",
    "        for batch_states, batch_actions in data_loader: \n",
    "            y_pred = model(batch_states.float())\n",
    "            loss   = criterion(y_pred, batch_actions.float()) \n",
    "            total_loss += loss.item() \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            b += 1 \n",
    "            \n",
    "            loss_list.append(loss.item())\n",
    "            \n",
    "        if itr % print_every==0:\n",
    "            print(f'Epoch {itr} Loss: {total_loss/b:.8f}')\n",
    "\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.12634795\n",
      "Epoch 100 Loss: 0.04952276\n",
      "Epoch 200 Loss: 0.01994292\n",
      "Epoch 300 Loss: 0.01192390\n",
      "Epoch 400 Loss: 0.01099494\n",
      "Epoch 500 Loss: 0.01045828\n",
      "Epoch 600 Loss: 0.01016442\n",
      "Epoch 700 Loss: 0.01005926\n",
      "Epoch 800 Loss: 0.00968910\n",
      "Epoch 900 Loss: 0.00948630\n",
      "Epoch 1000 Loss: 0.00934580\n",
      "Epoch 1100 Loss: 0.00931209\n",
      "Epoch 1200 Loss: 0.00901996\n",
      "Epoch 1300 Loss: 0.00871296\n",
      "Epoch 1400 Loss: 0.00862300\n",
      "Epoch 1500 Loss: 0.00873899\n",
      "Epoch 1600 Loss: 0.00846909\n",
      "Epoch 1700 Loss: 0.00830629\n",
      "Epoch 1800 Loss: 0.00812401\n",
      "Epoch 1900 Loss: 0.00797418\n",
      "Epoch 2000 Loss: 0.00781637\n",
      "Epoch 2100 Loss: 0.00752714\n",
      "Epoch 2200 Loss: 0.00749926\n",
      "Epoch 2300 Loss: 0.00725974\n",
      "Epoch 2400 Loss: 0.00708207\n",
      "Epoch 2500 Loss: 0.00710755\n",
      "Epoch 2600 Loss: 0.00683588\n",
      "Epoch 2700 Loss: 0.00729693\n",
      "Epoch 2800 Loss: 0.00671106\n",
      "Epoch 2900 Loss: 0.00678624\n",
      "Epoch 3000 Loss: 0.00656327\n"
     ]
    }
   ],
   "source": [
    "losses = train(bc, D_org_states, D_org_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:56)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:57)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/ns/mimicgen/mimicgen_envs/robosuite/robosuite/scripts/setup_macros.py (macros.py:58)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created environment with name Lift\n",
      "Action size is 7\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs modality: low_dim with keys: ['robot0_eef_pos']\n",
      "using obs modality: rgb with keys: []\n"
     ]
    }
   ],
   "source": [
    "env_meta=FileUtils.get_env_metadata_from_dataset(dataset_path)\n",
    "env = EnvUtils.create_env_from_metadata(\n",
    "    env_meta=env_meta, \n",
    "    render=False,            # no on-screen rendering\n",
    "    render_offscreen=True,   # off-screen rendering to support rendering video frames\n",
    ")\n",
    "dummy_spec = dict(  obs=dict( low_dim=[\"robot0_eef_pos\"], rgb=[], ),)\n",
    "ObsUtils.initialize_obs_utils_with_obs_specs(obs_modality_specs=dummy_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(model, env, rollout_horizon = 400, video_path=None, seed=40):\n",
    "    total_reward=0 \n",
    "    select_keys=['object', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos']\n",
    "\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    obs = env.reset()\n",
    "    state_dict = env.get_state()\n",
    "\n",
    "    #saving <obs,action> to be relabelled by expert\n",
    "    rollout_obss = []\n",
    "    rollout_actions=[]\n",
    " \n",
    "    for step_i in range(rollout_horizon):\n",
    "        select_obs=np.hstack( [obs[key] for key in select_keys] ) \n",
    "        state=torch.from_numpy(select_obs).float()\n",
    "        # state=state.to(device='cuda')\n",
    "\n",
    "        act = model(state).detach().cpu().numpy()\n",
    "\n",
    "        rollout_obss.append(state)\n",
    "        rollout_actions.append(act)\n",
    "\n",
    "        next_obs, r, done, _ = env.step(act)\n",
    "\n",
    "        # compute reward\n",
    "        total_reward += r\n",
    "        success = env.is_success()[\"task\"]\n",
    "\n",
    " \n",
    "        # break if done or if success\n",
    "        if done or success:\n",
    "            # print(f'stop: done={done} success={success}')\n",
    "            break\n",
    "\n",
    "        # update for next iter\n",
    "        obs = deepcopy(next_obs)\n",
    "\n",
    " \n",
    "    return total_reward, rollout_obss, rollout_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_rollout=20\n",
    "# s=0\n",
    "# for i in range(n_rollout):\n",
    "#     r,_,_=rollout(bc, env, video_path=None)\n",
    "#     s+=r\n",
    "#     print(f'Rollout {i} Success: {r}')\n",
    "\n",
    "# print(f'\\nAverage Reward: {s/n_rollout:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(bc.state_dict(), \"expert_lift_ph_95.pth\")\n",
    "\n",
    "# torch.save(bc, \"bc_lift_expert_full_ph.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expert_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load expert policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/AssistiveRoboticsUNH/bc_tutorial.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1964916/488421292.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expert_bc = torch.load(\"bc_lift_expert_full_ph.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RegNet(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (decoder): Linear(in_features=64, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expert_bc = torch.load(\"bc_lift_expert_full_ph.pth\")\n",
    "expert_bc = torch.load(\"/content/bc_tutorial/robomimic_tasks/bc_lift_expert_full_ph.pth\")\n",
    "expert_bc.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dagger itr: 0\n",
      "collecting 20 rollouts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:42<00:00,  5.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Reward: 0.25 total new: 6689\n",
      "\n",
      "Relabelling actions \n",
      "aggregating dataset... \n",
      "Updated dataset: (8377, 19) , (8377, 7)\n",
      "training ... \n",
      "Epoch 0 Loss: 0.04852031\n",
      "Epoch 100 Loss: 0.01301017\n",
      "Epoch 200 Loss: 0.00870638\n",
      "Epoch 300 Loss: 0.00695585\n",
      "Epoch 400 Loss: 0.00575945\n",
      "Epoch 500 Loss: 0.00482371\n",
      "Epoch 600 Loss: 0.00427983\n",
      "Epoch 700 Loss: 0.00384200\n",
      "Epoch 800 Loss: 0.00357729\n",
      "Epoch 900 Loss: 0.00334985\n",
      "Epoch 1000 Loss: 0.00316003\n",
      "Epoch 1100 Loss: 0.00301083\n",
      "Epoch 1200 Loss: 0.00288521\n",
      "Epoch 1300 Loss: 0.00272427\n",
      "Epoch 1400 Loss: 0.00260760\n",
      "Epoch 1500 Loss: 0.00249570\n",
      "Epoch 1600 Loss: 0.00240855\n",
      "Epoch 1700 Loss: 0.00230840\n",
      "Epoch 1800 Loss: 0.00226058\n",
      "Epoch 1900 Loss: 0.00218271\n",
      "Epoch 2000 Loss: 0.00213247\n",
      "Epoch 2100 Loss: 0.00208134\n",
      "Epoch 2200 Loss: 0.00205122\n",
      "Epoch 2300 Loss: 0.00200033\n",
      "Epoch 2400 Loss: 0.00198254\n",
      "Epoch 2500 Loss: 0.00192703\n",
      "Epoch 2600 Loss: 0.00192311\n",
      "Epoch 2700 Loss: 0.00186622\n",
      "Epoch 2800 Loss: 0.00187761\n",
      "Epoch 2900 Loss: 0.00185235\n",
      "Epoch 3000 Loss: 0.00180494\n",
      "\n",
      "dagger itr: 1\n",
      "collecting 20 rollouts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:33<00:00,  4.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Reward: 0.35 total new: 5823\n",
      "\n",
      "Relabelling actions \n",
      "aggregating dataset... \n",
      "Updated dataset: (14200, 19) , (14200, 7)\n",
      "training ... \n",
      "Epoch 0 Loss: 0.17036391\n",
      "Epoch 100 Loss: 0.00570416\n",
      "Epoch 200 Loss: 0.00444431\n",
      "Epoch 300 Loss: 0.00381565\n",
      "Epoch 400 Loss: 0.00343662\n",
      "Epoch 500 Loss: 0.00316255\n",
      "Epoch 600 Loss: 0.00296795\n",
      "Epoch 700 Loss: 0.00280661\n",
      "Epoch 800 Loss: 0.00271333\n",
      "Epoch 900 Loss: 0.00258774\n",
      "Epoch 1000 Loss: 0.00251942\n",
      "Epoch 1100 Loss: 0.00245590\n",
      "Epoch 1200 Loss: 0.00236516\n",
      "Epoch 1300 Loss: 0.00230745\n",
      "Epoch 1400 Loss: 0.00225643\n",
      "Epoch 1500 Loss: 0.00218715\n",
      "Epoch 1600 Loss: 0.00216913\n",
      "Epoch 1700 Loss: 0.00211289\n",
      "Epoch 1800 Loss: 0.00207446\n",
      "Epoch 1900 Loss: 0.00205388\n",
      "Epoch 2000 Loss: 0.00200443\n",
      "Epoch 2100 Loss: 0.00199523\n",
      "Epoch 2200 Loss: 0.00197539\n",
      "Epoch 2300 Loss: 0.00196592\n",
      "Epoch 2400 Loss: 0.00189338\n",
      "Epoch 2500 Loss: 0.00191083\n",
      "Epoch 2600 Loss: 0.00187657\n",
      "Epoch 2700 Loss: 0.00186087\n",
      "Epoch 2800 Loss: 0.00183245\n",
      "Epoch 2900 Loss: 0.00181395\n",
      "Epoch 3000 Loss: 0.00180557\n",
      "\n",
      "dagger itr: 2\n",
      "collecting 20 rollouts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:29<00:00,  4.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Reward: 0.35 total new: 5709\n",
      "\n",
      "Relabelling actions \n",
      "aggregating dataset... \n",
      "Updated dataset: (19909, 19) , (19909, 7)\n",
      "training ... \n",
      "Epoch 0 Loss: 0.01979364\n",
      "Epoch 100 Loss: 0.00363477\n",
      "Epoch 200 Loss: 0.00299972\n",
      "Epoch 300 Loss: 0.00270542\n",
      "Epoch 400 Loss: 0.00254873\n",
      "Epoch 500 Loss: 0.00245105\n",
      "Epoch 600 Loss: 0.00237050\n",
      "Epoch 700 Loss: 0.00229540\n",
      "Epoch 800 Loss: 0.00223223\n",
      "Epoch 900 Loss: 0.00217266\n",
      "Epoch 1000 Loss: 0.00213656\n",
      "Epoch 1100 Loss: 0.00212119\n",
      "Epoch 1200 Loss: 0.00208631\n",
      "Epoch 1300 Loss: 0.00204508\n",
      "Epoch 1400 Loss: 0.00201560\n",
      "Epoch 1500 Loss: 0.00200054\n",
      "Epoch 1600 Loss: 0.00198614\n",
      "Epoch 1700 Loss: 0.00196589\n",
      "Epoch 1800 Loss: 0.00192577\n",
      "Epoch 1900 Loss: 0.00193222\n",
      "Epoch 2000 Loss: 0.00190555\n",
      "Epoch 2100 Loss: 0.00186881\n",
      "Epoch 2200 Loss: 0.00188694\n",
      "Epoch 2300 Loss: 0.00185316\n",
      "Epoch 2400 Loss: 0.00184261\n",
      "Epoch 2500 Loss: 0.00182280\n",
      "Epoch 2600 Loss: 0.00181040\n",
      "Epoch 2700 Loss: 0.00179870\n",
      "Epoch 2800 Loss: 0.00178643\n",
      "Epoch 2900 Loss: 0.00178275\n",
      "Epoch 3000 Loss: 0.00177958\n",
      "\n",
      "dagger itr: 3\n",
      "collecting 20 rollouts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:08<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Reward: 0.55 total new: 4183\n",
      "\n",
      "Relabelling actions \n",
      "aggregating dataset... \n",
      "Updated dataset: (24092, 19) , (24092, 7)\n",
      "training ... \n",
      "Epoch 0 Loss: 0.05826057\n",
      "Epoch 100 Loss: 0.00429957\n",
      "Epoch 200 Loss: 0.00347803\n",
      "Epoch 300 Loss: 0.00316219\n",
      "Epoch 400 Loss: 0.00294381\n",
      "Epoch 500 Loss: 0.00282564\n",
      "Epoch 600 Loss: 0.00271598\n",
      "Epoch 700 Loss: 0.00261980\n",
      "Epoch 800 Loss: 0.00254547\n",
      "Epoch 900 Loss: 0.00250291\n",
      "Epoch 1000 Loss: 0.00246020\n",
      "Epoch 1100 Loss: 0.00240868\n",
      "Epoch 1200 Loss: 0.00236077\n",
      "Epoch 1300 Loss: 0.00234381\n",
      "Epoch 1400 Loss: 0.00229690\n",
      "Epoch 1500 Loss: 0.00226088\n",
      "Epoch 1600 Loss: 0.00223899\n",
      "Epoch 1700 Loss: 0.00223163\n",
      "Epoch 1800 Loss: 0.00221879\n",
      "Epoch 1900 Loss: 0.00220434\n",
      "Epoch 2000 Loss: 0.00217011\n",
      "Epoch 2100 Loss: 0.00216394\n",
      "Epoch 2200 Loss: 0.00212223\n",
      "Epoch 2300 Loss: 0.00212612\n",
      "Epoch 2400 Loss: 0.00210277\n",
      "Epoch 2500 Loss: 0.00209009\n",
      "Epoch 2600 Loss: 0.00209287\n",
      "Epoch 2700 Loss: 0.00207503\n",
      "Epoch 2800 Loss: 0.00205937\n",
      "Epoch 2900 Loss: 0.00206275\n",
      "Epoch 3000 Loss: 0.00203205\n",
      "\n",
      "dagger itr: 4\n",
      "collecting 20 rollouts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:38<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Reward: 0.85 total new: 2064\n",
      "\n",
      "Relabelling actions \n",
      "aggregating dataset... \n",
      "Updated dataset: (26156, 19) , (26156, 7)\n",
      "training ... \n",
      "Epoch 0 Loss: 0.01849106\n",
      "Epoch 100 Loss: 0.00283443\n",
      "Epoch 200 Loss: 0.00259822\n",
      "Epoch 300 Loss: 0.00248698\n",
      "Epoch 400 Loss: 0.00242445\n",
      "Epoch 500 Loss: 0.00235309\n",
      "Epoch 600 Loss: 0.00230224\n",
      "Epoch 700 Loss: 0.00228584\n",
      "Epoch 800 Loss: 0.00226342\n",
      "Epoch 900 Loss: 0.00223026\n",
      "Epoch 1000 Loss: 0.00220018\n",
      "Epoch 1100 Loss: 0.00218036\n",
      "Epoch 1200 Loss: 0.00218770\n",
      "Epoch 1300 Loss: 0.00216330\n",
      "Epoch 1400 Loss: 0.00213479\n",
      "Epoch 1500 Loss: 0.00212338\n",
      "Epoch 1600 Loss: 0.00210447\n",
      "Epoch 1700 Loss: 0.00209640\n",
      "Epoch 1800 Loss: 0.00208735\n",
      "Epoch 1900 Loss: 0.00206631\n",
      "Epoch 2000 Loss: 0.00207832\n",
      "Epoch 2100 Loss: 0.00205897\n",
      "Epoch 2200 Loss: 0.00206263\n",
      "Epoch 2300 Loss: 0.00203268\n",
      "Epoch 2400 Loss: 0.00202370\n",
      "Epoch 2500 Loss: 0.00204858\n",
      "Epoch 2600 Loss: 0.00203207\n",
      "Epoch 2700 Loss: 0.00200695\n",
      "Epoch 2800 Loss: 0.00199966\n",
      "Epoch 2900 Loss: 0.00200836\n",
      "Epoch 3000 Loss: 0.00199141\n"
     ]
    }
   ],
   "source": [
    "#initial dataset.\n",
    "D_s = D_org_states\n",
    "D_a = D_org_actions\n",
    "\n",
    "n_rollout=20\n",
    " \n",
    "for dagger_itr in range(5): \n",
    "    print(f\"\\ndagger itr: {dagger_itr}\")\n",
    "    print(f\"collecting {n_rollout} rollouts\")\n",
    "\n",
    "    s, D_s_new, D_a_new = 0, [], [] \n",
    "    for i in tqdm( range(n_rollout) ):\n",
    "        r,ss,aa=rollout(bc, env, video_path=None)\n",
    "        D_s_new.extend(ss)\n",
    "        D_a_new.extend(aa)\n",
    "        s+=r\n",
    "        # print(f'Rollout {i} Success: {r} new_ss: {len(ss)}')\n",
    "    print(f'\\nAverage Reward: {s/n_rollout:.2f} total new: {len(D_s_new)}')\n",
    "\n",
    "    D_s_new = np.stack(D_s_new)\n",
    "    D_a_new = np.stack(D_a_new)\n",
    "\n",
    "    print(\"\\nRelabelling actions \")\n",
    "    state=torch.from_numpy(D_s_new).float()\n",
    "    D_a_new = expert_bc(state).detach().cpu().numpy()  #relabelled action using expert\n",
    "\n",
    "\n",
    "    print('aggregating dataset... ')\n",
    "    D_s = np.vstack([D_s , D_s_new])\n",
    "    D_a = np.vstack([D_a, D_a_new])\n",
    "\n",
    "    print(f'Updated dataset: {D_s.shape} , {D_a.shape}')\n",
    "\n",
    "    print('training ... ')\n",
    "    losses = train(bc, D_s, D_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollout 0 Success: 1.0\n",
      "Rollout 1 Success: 1.0\n",
      "Rollout 2 Success: 0.0\n",
      "Rollout 3 Success: 1.0\n",
      "Rollout 4 Success: 1.0\n",
      "Rollout 5 Success: 1.0\n",
      "Rollout 6 Success: 1.0\n",
      "Rollout 7 Success: 1.0\n",
      "Rollout 8 Success: 1.0\n",
      "Rollout 9 Success: 0.0\n",
      "Rollout 10 Success: 1.0\n",
      "Rollout 11 Success: 1.0\n",
      "Rollout 12 Success: 1.0\n",
      "Rollout 13 Success: 1.0\n",
      "Rollout 14 Success: 0.0\n",
      "Rollout 15 Success: 1.0\n",
      "Rollout 16 Success: 0.0\n",
      "Rollout 17 Success: 1.0\n",
      "Rollout 18 Success: 1.0\n",
      "Rollout 19 Success: 1.0\n",
      "\n",
      "Average Reward: 0.80\n"
     ]
    }
   ],
   "source": [
    "n_rollout=20\n",
    "s=0\n",
    "for i in range(n_rollout):\n",
    "    r,_,_=rollout(bc, env, video_path=None, seed=42)\n",
    "    s+=r\n",
    "    print(f'Rollout {i} Success: {r}')\n",
    "\n",
    "print(f'\\nAverage Reward: {s/n_rollout:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: try with 3 different seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimicgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
