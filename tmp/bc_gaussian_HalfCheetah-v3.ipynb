{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import time \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import gym \n",
    "import pickle \n",
    "from replay_memory import Memory\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions.transformed_distribution import TransformedDistribution\n",
    "from torch.distributions.transforms import TanhTransform\n",
    "from torch.optim import Adam\n",
    "import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ns/anaconda3/lib/python3.9/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name='HalfCheetah-v3'\n",
    "data_path = \"expert_data/HalfCheetah-v3_10_9498.pkl\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expert data loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000, 17), (10000, 6), (10000, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(data_path, \"rb\") as f:\n",
    "    data_good = pickle.load(f)\n",
    "print('expert data loaded')\n",
    "\n",
    "data_good=data_good[:20]\n",
    "\n",
    "good_obs=[]\n",
    "good_acts=[]\n",
    "good_rews=[]\n",
    "\n",
    "for traj in data_good: \n",
    "    s,a,r=traj  \n",
    "\n",
    "    good_obs.append(s)\n",
    "    good_acts.append(a)\n",
    "    good_rews.append(r)\n",
    "\n",
    "good_obs=np.vstack(good_obs)\n",
    "good_acts=np.vstack(good_acts)\n",
    "good_rews=np.vstack(good_rews) \n",
    "\n",
    "good_obs.shape, good_acts.shape, good_rews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 17]), torch.Size([64, 6]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = DataLoader( list(zip(good_obs, good_acts)), batch_size=64, shuffle=True)\n",
    "\n",
    "batch=next(iter(data_loader))\n",
    "states,actions = batch\n",
    "states.shape,actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 6\n"
     ]
    }
   ],
   "source": [
    "action_dim=actions.shape[1]\n",
    "state_dim=states.shape[1]\n",
    "print(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, size=32):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim,size),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(size,size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size,size),\n",
    "            nn.ReLU() \n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    " \n",
    "class GaussianPolicy(MLP):\n",
    "    def __init__(self, input_dim, output_dim, hidden_size=64):\n",
    "        super(GaussianPolicy, self).__init__(input_dim, hidden_size) \n",
    "        self.mean = nn.Linear(hidden_size, output_dim) \n",
    "        self.log_std_layer = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.net(state)\n",
    "        mean = self.mean(x) \n",
    "        log_std = self.log_std_layer(x)  # Predict log_std using a linear layer\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "\n",
    "policy = GaussianPolicy(state_dim, action_dim, 64)\n",
    "optimizer = Adam(policy.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 5.906\n",
      "Epoch 200 Loss: -8.794\n",
      "Epoch 400 Loss: -10.227\n",
      "Epoch 600 Loss: -11.098\n",
      "Epoch 800 Loss: -11.776\n",
      "Epoch 1000 Loss: -12.245\n",
      "Epoch 1200 Loss: -12.730\n",
      "Epoch 1400 Loss: -13.180\n",
      "Epoch 1600 Loss: -13.477\n",
      "Epoch 1800 Loss: -13.851\n",
      "Epoch 2000 Loss: -14.055\n",
      "Epoch 2200 Loss: -14.189\n",
      "Epoch 2400 Loss: -14.478\n",
      "Epoch 2600 Loss: -14.837\n",
      "Epoch 2800 Loss: -14.890\n",
      "Epoch 3000 Loss: -15.071\n",
      "Epoch 3200 Loss: -15.190\n",
      "Epoch 3400 Loss: -15.395\n",
      "Epoch 3600 Loss: -15.508\n",
      "Epoch 3800 Loss: -15.683\n"
     ]
    }
   ],
   "source": [
    "num_epochs=4_000\n",
    "for epoch in range(num_epochs): \n",
    "    total_loss=0\n",
    "    b=0\n",
    "    for states, actions in data_loader: \n",
    "        means, stds = policy(states.float())\n",
    "        dist = Normal(means, stds)\n",
    "        log_probs = dist.log_prob(actions).sum(dim=-1)\n",
    "        loss = -log_probs.mean()\n",
    "\n",
    "        total_loss += loss.item() \n",
    "        b=b+1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % (num_epochs//20)==0:\n",
    "        print(f'Epoch {epoch} Loss: {total_loss/b:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ns/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment HalfCheetah-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/ns/anaconda3/lib/python3.9/site-packages/gym/envs/mujoco/mujoco_env.py:190: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "obs,info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, policy, is_close=True, is_render=True, max_step=1000): \n",
    "    obs,info = env.reset()\n",
    "    dones=False\n",
    "    total_r=0\n",
    "    step=0\n",
    "    while not dones: \n",
    "        step+=1\n",
    "        states=torch.Tensor(obs[None]).to(device)\n",
    "        means, stds = policy(states.float())\n",
    "        action = means.detach().cpu().numpy()[0]\n",
    "\n",
    "        obs, rewards, done, s, info = env.step(action)\n",
    "        total_r +=rewards  \n",
    "        if done:\n",
    "            break\n",
    "        if step>max_step:\n",
    "            # print('max step reached')\n",
    "            break\n",
    "        # elif s:\n",
    "        #     print('solved!')\n",
    "        #     break\n",
    "    if is_close:\n",
    "        env.close()\n",
    "    return {'reward':total_r, 'step':step-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reward': 139.33717097829347, 'step': 1000}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "play(env, policy, is_close=True, is_render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode #0 reward: 737.12\n",
      "episode #1 reward: 148.10\n",
      "episode #2 reward: -1145.00\n",
      "episode #3 reward: 716.50\n",
      "episode #4 reward: -896.93\n",
      "episode #5 reward: -1313.66\n",
      "episode #6 reward: -559.43\n",
      "episode #7 reward: -1220.03\n",
      "episode #8 reward: -361.22\n",
      "episode #9 reward: 341.29\n",
      "episode #10 reward: -1017.39\n",
      "episode #11 reward: 599.46\n",
      "episode #12 reward: -24.44\n",
      "episode #13 reward: -1100.42\n",
      "episode #14 reward: -910.69\n",
      "episode #15 reward: -917.96\n",
      "episode #16 reward: 610.78\n",
      "episode #17 reward: -1107.15\n",
      "episode #18 reward: 132.76\n",
      "episode #19 reward: -386.84\n",
      "\n",
      " score: -383.76 +- 709.47\n"
     ]
    }
   ],
   "source": [
    "scores=[]\n",
    "n_trajectory=20\n",
    "for i in range(n_trajectory):\n",
    "    stats=play(env, policy, is_close=True, is_render=False)\n",
    "    rewards=stats['reward']\n",
    "    print(f'episode #{i} reward: {rewards:0.2f}')\n",
    "    scores.append(rewards)\n",
    "\n",
    "print(f'\\n score: {np.mean(scores):0.2f} +- {np.std(scores):0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imitation-dice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
